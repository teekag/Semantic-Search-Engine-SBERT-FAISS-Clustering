{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the core functionality of the semantic search engine:\n",
    "1. Loading sample data\n",
    "2. Embedding documents using SBERT\n",
    "3. Indexing embeddings with FAISS\n",
    "4. Searching for semantically similar documents\n",
    "5. Visualizing the embedding space and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Add the src directory to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our modules\n",
    "from src.embedder import Embedder\n",
    "from src.vector_store import VectorStore\n",
    "from src.clusterer import Clusterer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Sample Data\n",
    "\n",
    "For this demo, we'll use the 20 Newsgroups dataset, which contains news articles from 20 different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['comp.graphics', 'sci.med', 'rec.autos', 'talk.politics.guns']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create document dictionaries\n",
    "documents = []\n",
    "for i, (text, target) in enumerate(zip(newsgroups.data, newsgroups.target)):\n",
    "    # Clean up the text a bit\n",
    "    text = text.strip()\n",
    "    if len(text) > 100:  # Filter out very short documents\n",
    "        documents.append({\n",
    "            \"id\": i,\n",
    "            \"text\": text,\n",
    "            \"category\": newsgroups.target_names[target],\n",
    "            \"category_id\": target\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {len(categories)} categories\")\n",
    "\n",
    "# Display a sample document\n",
    "sample_idx = np.random.randint(0, len(documents))\n",
    "sample_doc = documents[sample_idx]\n",
    "print(f\"\\nSample document (category: {sample_doc['category']}):\\n\")\n",
    "print(sample_doc['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embed Documents using SBERT\n",
    "\n",
    "Now we'll use our Embedder class to convert the documents into dense vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the embedder\n",
    "embedder = Embedder(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extract the text from documents\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedder.embed_texts(texts)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1]}\")\n",
    "print(f\"Sample embedding (first 5 dimensions): {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Index Embeddings with FAISS\n",
    "\n",
    "Next, we'll use our VectorStore class to index the embeddings for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the vector store\n",
    "vector_store = VectorStore(\n",
    "    dimension=embedder.get_embedding_dimension(),\n",
    "    index_type=\"Flat\",  # Use Flat for exact search\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "# Add the embeddings to the index\n",
    "vector_store.add(embeddings, documents)\n",
    "\n",
    "# Display index info\n",
    "index_info = vector_store.get_index_info()\n",
    "for key, value in index_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search for Similar Documents\n",
    "\n",
    "Now we can search for documents that are semantically similar to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define some test queries\n",
    "test_queries = [\n",
    "    \"How to treat a high fever\",\n",
    "    \"The best sports cars on the market\",\n",
    "    \"Rendering 3D graphics with ray tracing\",\n",
    "    \"Second amendment rights and gun control laws\"\n",
    "]\n",
    "\n",
    "# Function to perform a search and display results\n",
    "def search_and_display(query, k=5):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = embedder.embed_query(query)\n",
    "    \n",
    "    # Search for similar documents\n",
    "    results = vector_store.search(query_embedding, k=k)\n",
    "    \n",
    "    # Display results\n",
    "    for i, (doc, sim) in enumerate(zip(results[\"documents\"], results[\"similarities\"])):\n",
    "        print(f\"{i+1}. [{doc['category']}] (Similarity: {sim:.4f})\")\n",
    "        print(f\"   {doc['text'][:200].replace(chr(10), ' ')}...\\n\")\n",
    "\n",
    "# Test each query\n",
    "for query in test_queries:\n",
    "    search_and_display(query)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Documents\n",
    "\n",
    "Let's use our Clusterer class to discover semantic groupings in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the clusterer\n",
    "clusterer = Clusterer(algorithm=\"kmeans\")\n",
    "\n",
    "# Fit the clusterer (use the number of actual categories)\n",
    "labels = clusterer.fit(embeddings, documents, n_clusters=len(categories))\n",
    "\n",
    "# Evaluate the clustering\n",
    "score = clusterer.evaluate()\n",
    "print(f\"Silhouette score: {score:.4f}\")\n",
    "\n",
    "# Get cluster info\n",
    "cluster_info = clusterer.get_cluster_info()\n",
    "print(f\"\\nCluster counts:\")\n",
    "for label, count in cluster_info[\"cluster_counts\"].items():\n",
    "    print(f\"Cluster {label}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Embeddings and Clusters\n",
    "\n",
    "Let's visualize the document embeddings in 2D space using UMAP and color them by category and cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reduce dimensions for visualization\n",
    "reduced_embeddings = clusterer.reduce_dimensions(n_components=2)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'x': reduced_embeddings[:, 0],\n",
    "    'y': reduced_embeddings[:, 1],\n",
    "    'category': [doc['category'] for doc in documents],\n",
    "    'cluster': labels\n",
    "})\n",
    "\n",
    "# Plot by true category\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(data=plot_df, x='x', y='y', hue='category', palette='viridis', alpha=0.7)\n",
    "plt.title('Document Embeddings by Category')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot by predicted cluster\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(data=plot_df, x='x', y='y', hue='cluster', palette='tab10', alpha=0.7)\n",
    "plt.title('Document Embeddings by Cluster')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Cluster Quality\n",
    "\n",
    "Let's analyze how well our clusters align with the true categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a contingency table of category vs. cluster\n",
    "contingency = pd.crosstab(plot_df['category'], plot_df['cluster'])\n",
    "\n",
    "# Display the table\n",
    "print(\"Category vs. Cluster Contingency Table:\")\n",
    "print(contingency)\n",
    "\n",
    "# Plot a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency, annot=True, cmap='YlGnBu', fmt='d')\n",
    "plt.title('Category vs. Cluster Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate cluster purity\n",
    "def calculate_purity(contingency):\n",
    "    return np.sum(np.max(contingency.values, axis=0)) / np.sum(contingency.values)\n",
    "\n",
    "purity = calculate_purity(contingency)\n",
    "print(f\"Cluster purity: {purity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Models\n",
    "\n",
    "Finally, let's save our models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directories\n",
    "output_dir = \"../outputs/models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the vector store\n",
    "vector_store_path = vector_store.save(output_dir, filename=\"newsgroups_vector_store\")\n",
    "\n",
    "# Save the clusterer\n",
    "clusterer_path = clusterer.save(output_dir, filename=\"newsgroups_clusterer\")\n",
    "\n",
    "print(f\"Models saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the core functionality of our semantic search engine:\n",
    "\n",
    "1. We loaded sample documents from the 20 Newsgroups dataset\n",
    "2. We embedded the documents using SBERT\n",
    "3. We indexed the embeddings using FAISS\n",
    "4. We performed semantic searches with natural language queries\n",
    "5. We clustered the documents to discover semantic groupings\n",
    "6. We visualized the embedding space and analyzed cluster quality\n",
    "\n",
    "This demonstrates the power of embedding-based semantic search and clustering for understanding and organizing text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
