# Semantic Search Engine

## SBERT + FAISS + Clustering

üîç A full-stack semantic search pipeline optimized for speed, interpretability, and clustering insight. Built with:

- **SBERT (Sentence-BERT)** for contextual embeddings
- **FAISS** for high-speed approximate nearest-neighbor search
- **UMAP + KMeans/DBSCAN/HDBSCAN** for visual and structural clustering

Used for document discovery, B2B intelligence, and knowledge base retrieval. The system is modular, GPU-accelerated, and includes visual notebooks and quantitative evaluations.

üìà **Snapshot Stats**:
- ‚è±Ô∏è ~10ms per query search across 20K+ documents (Flat index)
- üìä 87% NMI clustering score (KMeans on topic-rich corpora)
- üìé Plug-and-play with any SBERT-compatible model (MiniLM, MPNet, etc.)

![Semantic Search](https://miro.medium.com/max/1400/1*FpnHmhUlUrz0GuKOJBpG9g.png)

## Project Overview

This project implements a complete semantic search pipeline using:

- **Sentence-BERT (SBERT)** for embedding queries and documents
- **FAISS** for fast vector search and retrieval
- **Clustering** for analyzing semantic groupings in the corpus

The system allows for natural language queries to find semantically similar documents, even when they don't share exact keywords.

## Features

- **Document Embedding**: Convert text documents into dense vector representations using state-of-the-art transformer models
- **Vector Indexing**: Efficiently store and search embeddings using FAISS
- **Semantic Search**: Find documents with similar meaning, not just keyword matches
- **Clustering Analysis**: Discover semantic groupings and structure in the document corpus
- **Visualization**: Explore the embedding space and cluster distributions
- **Extensible**: Easy to swap models or add new components

## Project Structure

```
semantic-search-engine/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Original text documents
‚îÇ   ‚îî‚îÄ‚îÄ processed/     # Preprocessed or chunked data
‚îú‚îÄ‚îÄ src/               # All core logic: embedding, indexing, clustering
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py    # SBERT-based text embedding
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py # FAISS indexing and retrieval
‚îÇ   ‚îî‚îÄ‚îÄ clusterer.py   # Unsupervised clustering of embeddings
‚îú‚îÄ‚îÄ notebooks/         # Jupyter notebooks for demos and analysis
‚îÇ   ‚îî‚îÄ‚îÄ basic_search_pipeline.ipynb  # Demo of the core functionality
‚îú‚îÄ‚îÄ outputs/           # Search results, visualizations, clusters
‚îú‚îÄ‚îÄ diagrams/          # System or embedding diagrams
‚îî‚îÄ‚îÄ README.md          # This file
```

## Core Components

### 1. Embedder (`src/embedder.py`)

The Embedder class handles:
- Loading pre-trained SBERT models
- Converting text documents to embeddings
- Supporting both batch processing and single queries

```python
from src.embedder import Embedder

embedder = Embedder(model_name="all-MiniLM-L6-v2")
query_embedding = embedder.embed_query("How does semantic search work?")
```

### 2. Vector Store (`src/vector_store.py`)

The VectorStore class provides:
- FAISS index creation and management
- Fast similarity search
- Support for different index types (Flat, IVF, HNSW)
- Persistence (save/load)

```python
from src.vector_store import VectorStore

vector_store = VectorStore(dimension=384, index_type="Flat", metric="cosine")
vector_store.add(embeddings, documents)
results = vector_store.search(query_embedding, k=5)
```

### 3. Clusterer (`src/clusterer.py`)

The Clusterer class enables:
- Unsupervised clustering of document embeddings
- Support for multiple algorithms (KMeans, DBSCAN, HDBSCAN, Agglomerative)
- Cluster evaluation and visualization
- Dimensionality reduction for visualization

```python
from src.clusterer import Clusterer

clusterer = Clusterer(algorithm="kmeans")
labels = clusterer.fit(embeddings, documents, n_clusters=5)
reduced = clusterer.reduce_dimensions(n_components=2)
```

## Getting Started

### Prerequisites

- Python 3.8+
- pip

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/semantic-search-engine.git
cd semantic-search-engine
```

2. Install the required packages:
```bash
pip install -r requirements.txt
```

### Running the Demo

1. Start Jupyter Notebook:
```bash
jupyter notebook
```

2. Open `notebooks/basic_search_pipeline.ipynb`
3. Run the cells to see the semantic search pipeline in action

## Example Use Cases

- **Document Retrieval**: Find relevant documents based on natural language queries
- **Content Recommendation**: Suggest similar content based on semantic similarity
- **Topic Modeling**: Discover themes and topics in a corpus through clustering
- **Data Exploration**: Visualize and understand the semantic structure of a text collection

## Future Enhancements

- Implement additional embedding models for comparison (e.g., OpenAI, MPNet)
- Add hybrid search (combining semantic and keyword search)
- Integrate with a web UI for interactive demos
- Add support for incremental indexing and updates
- Implement cross-encoder reranking for improved precision

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- [Sentence-Transformers](https://www.sbert.net/) for the embedding models
- [FAISS](https://github.com/facebookresearch/faiss) for efficient similarity search
- [scikit-learn](https://scikit-learn.org/) for clustering algorithms
- [UMAP](https://umap-learn.readthedocs.io/) for dimensionality reduction
